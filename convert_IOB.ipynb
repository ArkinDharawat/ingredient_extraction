{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2cc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_text_entities(path):\n",
    "    with open(path, 'r') as fobj:\n",
    "        d = json.load(fobj)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c878f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ea7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(tokens):\n",
    "    if len(tokens) == 1:\n",
    "        return ['B-ing']\n",
    "    else:\n",
    "        labels = ['B-ing']\n",
    "        for t in range(1, len(tokens)):\n",
    "            labels.append('I-ing')\n",
    "        return labels\n",
    "\n",
    "def getO_label(tokens):\n",
    "    labels = []\n",
    "    for t in tokens:\n",
    "        labels.append('O')\n",
    "    return labels\n",
    "\n",
    "def label_ingredients(txt, labels):\n",
    "    tokens = []\n",
    "    t_lables = []\n",
    "    prev = 0\n",
    "    for start,end,_ in labels['entities']:\n",
    "        pre_str = txt[prev:start]\n",
    "        pre_tokens = word_tokenize(pre_str)\n",
    "        pre_labels = getO_label(pre_tokens)\n",
    "        tokens.extend(pre_tokens)\n",
    "        t_lables.extend(pre_labels)\n",
    "        \n",
    "        cur_str = txt[start:end]\n",
    "        cur_tokens = word_tokenize(cur_str)\n",
    "        cur_labels = get_label(cur_tokens)\n",
    "        tokens.extend(cur_tokens)\n",
    "        t_lables.extend(cur_labels)\n",
    "        \n",
    "        prev = end\n",
    "        \n",
    "    return tokens, t_lables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f61095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "Puree all ingredients in blender on high until smooth. Serve immediately.\n",
      "247\n",
      "Place all ingredients in a blender and blend until smooth.\n",
      "696\n",
      "Mix ingredients in microwaveable bowl. Microwave on HIGH 5 minutes or until VELVEETA is completely melted, stirring after 3 minutes.\n",
      "1040\n",
      "Puree all ingredients in a blender until smooth.\n",
      "1308\n",
      "Puree all ingredients in a blender until smooth.\n",
      "2862\n",
      "Blend all ingredients together, very slowly increasing speed from low to high. Blend for 1 1/2 minutes at high.\n",
      "2906\n",
      "Place all ingredients into blender and blend on high until smooth.\n",
      "3174\n",
      "Combine all ingredients in a large bag and shake to mix. Divide and store in individual baggies for a fast treat.\n",
      "3651\n",
      "Add all ingredients to blender. Blend on high until smooth. Enjoy!\n",
      "4549\n",
      "Mix ingredients in microwaveable bowl. Microwave on HIGH 5 minutes or until VELVEETA is completely melted, stirring after 3 minutes.\n"
     ]
    }
   ],
   "source": [
    "data = get_text_entities('combined_valid.json')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    txt, labels = data[i]\n",
    "\n",
    "    tokens, annotates  = label_ingredients(txt, labels)\n",
    "    pos_tags = list(map(lambda x: x[1], pos_tag(tokens)))\n",
    "    if len(tokens) == 0:\n",
    "        # nothing here\n",
    "        print(i)\n",
    "        print(txt)\n",
    "        continue\n",
    "    \n",
    "    data_dict = {}\n",
    "    data_dict['Sentence'] = [f'Sentence: {i + 1}'] + [''] * (len(tokens)-1)\n",
    "    data_dict['Word'] = tokens\n",
    "    data_dict['POS'] = pos_tags\n",
    "    data_dict['Tag'] = annotates\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_dict, orient='columns')\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14cddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48769ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('combined_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1dc43f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458258, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aaf1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "682_venv",
   "language": "python",
   "name": "682_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

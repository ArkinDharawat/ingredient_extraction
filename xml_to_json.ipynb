{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.8\r\n"
     ]
    }
   ],
   "source": [
    "!python -V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/foodbase/FoodBase_curated.xml', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "Bs_data = BeautifulSoup(data, \"xml\")\n",
    "\n",
    "b_doc = Bs_data.find_all('document')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(type(b_doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<annotation id=\"1\">\n",
      "<location length=\"12\" offset=\"3\"/>\n",
      "<text>cream cheese</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.e [Dairy produce];AG.01.e.02 [Cheese];AG.01.n [Dishes and prepared food];AG.01.n.18 [Preserve];</infon>\n",
      "</annotation>, <annotation id=\"2\">\n",
      "<location length=\"4\" offset=\"6\"/>\n",
      "<text>beef</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.d.03 [Beef];</infon>\n",
      "</annotation>, <annotation id=\"3\">\n",
      "<location length=\"6\" offset=\"8\"/>\n",
      "<text>olives</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.h.01.e [Fruit containing stone];</infon>\n",
      "</annotation>, <annotation id=\"4\">\n",
      "<location length=\"5\" offset=\"10\"/>\n",
      "<text>onion</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.h.02.e [Onion/leek/garlic];</infon>\n",
      "</annotation>, <annotation id=\"5\">\n",
      "<location length=\"20\" offset=\"13\"/>\n",
      "<text>Worcestershire sauce</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.h [Fruit and vegetables];AG.01.l.04 [Sauce/dressing];</infon>\n",
      "</annotation>, <annotation id=\"6\">\n",
      "<location length=\"7\" offset=\"63\"/>\n",
      "<text>walnuts</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.h.01.f [Nut];</infon>\n",
      "</annotation>, <annotation id=\"7\">\n",
      "<location length=\"11\" offset=\"67\"/>\n",
      "<text>cheese ball</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.e.02 [Cheese];AG.01.n.18 [Preserve];</infon>\n",
      "</annotation>, <annotation id=\"8\">\n",
      "<location length=\"7\" offset=\"71\"/>\n",
      "<text>walnuts</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.h.01.f [Nut];</infon>\n",
      "</annotation>, <annotation id=\"9\">\n",
      "<location length=\"11\" offset=\"78\"/>\n",
      "<text>cheese ball</text>\n",
      "<infon key=\"semantic_tags\"> AG.01.e.02 [Cheese];AG.01.n.18 [Preserve];</infon>\n",
      "</annotation>]\n"
     ]
    }
   ],
   "source": [
    "b_annotations = b_doc[0].find_all(\"annotation\")\n",
    "print(b_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_occurences(text, doc):\n",
    "    return [m.start() for m in re.finditer(text, doc)]\n",
    "\n",
    "# def get_tokens(sent):\n",
    "#     tokens = word_tokenize(sent)\n",
    "#     token_index = []\n",
    "#     cur_indx = 0\n",
    "#     for i in range(0, len(tokens)-1):\n",
    "#         token = tokens[i]\n",
    "#         token_index.append(cur_indx)\n",
    "#         cur_indx += len(token) + (0 if tokens[i+1] in [',', '.'] else 1)\n",
    "    \n",
    "#     return tokens, token_index\n",
    "\n",
    "def get_tokens(txt):\n",
    "    tokens = word_tokenize(txt)\n",
    "    offset = 0\n",
    "    token_indx_tuples = []\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        token_indx_tuples.append((token, offset))\n",
    "        offset += len(token)\n",
    "    \n",
    "    return token_indx_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "i = 0\n",
    "for docs in b_doc:\n",
    "    infon = docs.find(\"infon\", {\"key\": \"full_text\"})\n",
    "    document = infon.get_text().strip()\n",
    "    data.append([])\n",
    "    data[i].append(document)\n",
    "    data[i].append({'entities': []})\n",
    "    current_end = 0\n",
    "    token_indx_tuples = get_tokens(document)\n",
    "    for anots in docs.find_all(\"annotation\"):\n",
    "        word = anots.find(\"text\").get_text()\n",
    "        place = anots.find(\"location\")\n",
    "        offset = place[\"offset\"] # find the first occurence of this\n",
    "        token, starting_index = token_indx_tuples[int(offset) - 1]\n",
    "        length = place[\"length\"]\n",
    "        end = int(starting_index) + int(length)\n",
    "        # print(document[starting_index:end])\n",
    "        data[i][1][\"entities\"].append([starting_index, end, \"INGREDIENT\"])\n",
    "        current_end = end\n",
    "    i+=1\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"FoodBase_curated_fixed.json\", 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mix the cream cheese, beef, olives, onion, and Worcestershire sauce together in a bowl until evenly blended. Keeping the mixture in the bowl, scrape it into a semi-ball shape. Cover, and refrigerate until firm, at least 2 hours. Place a large sheet of waxed paper on a flat surface. Sprinkle with walnuts. Roll the cheese ball in the walnuts until completely covered. Transfer the cheese ball to a serving plate, or rewrap with waxed paper and refrigerate until needed.', {'entities': [[8, 20, 'INGREDIENT'], [22, 26, 'INGREDIENT'], [28, 34, 'INGREDIENT'], [36, 41, 'INGREDIENT'], [47, 67, 'INGREDIENT'], [297, 304, 'INGREDIENT'], [315, 326, 'INGREDIENT'], [334, 341, 'INGREDIENT'], [381, 392, 'INGREDIENT']]}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"FoodBase_curated_fixed.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water\n",
      "sun-dried tomatoes\n",
      "water\n",
      "cream cheese\n",
      "garlic\n",
      "provolone cheese\n",
      "pesto\n",
      "cream cheese\n",
      "garlic mixture\n",
      "sun-dried tomatoes\n",
      "provolone\n",
      "pesto\n",
      "cream cheese\n",
      "garlic mixture\n",
      "sun-dried tomatoes\n",
      "provolone cheese\n",
      "excess oil\n"
     ]
    }
   ],
   "source": [
    "doc, entites = data[2]\n",
    "for left, right, label in entites['entities']:\n",
    "    print(doc[left:right])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "682_venv",
   "language": "python",
   "name": "682_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
